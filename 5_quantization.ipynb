{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Gibur13/Digital-Architecture-for-Distance-Based-Sorting/blob/main/5_quantization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLNHtJrAqKWo"
      },
      "source": [
        "# **5. Quantization**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h5kS114ooq-0"
      },
      "source": [
        "## 5.0 Setup Capabilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zjOhFCIkxVfL",
        "outputId": "7dd84a9b-10c8-4767-8c67-a445641e2c21",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ],
      "source": [
        "# Mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "hoEUDK6KxVfM"
      },
      "outputs": [],
      "source": [
        "# Make sure your token is stored in a txt file at the location below.\n",
        "# This way there is no risk that you will push it to your repo\n",
        "# Never share your token with anyone, it is basically your github password!\n",
        "with open('/content/gdrive/MyDrive/ece5545/token.txt') as f:\n",
        "    token = f.readline().strip()\n",
        "# Use another file to store your github username\n",
        "with open('/content/gdrive/MyDrive/ece5545/git_username.txt') as f:\n",
        "    handle = f.readline().strip()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "lXP6DjaqxVfM",
        "outputId": "2383ce5c-2cc5-432d-ca0a-b9278b054f8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mkdir: cannot create directory ‘/content/gdrive/MyDrive/ece5545’: File exists\n",
            "/content/gdrive/MyDrive/ece5545\n",
            "fatal: destination path 'a2-Gibur13' already exists and is not an empty directory.\n",
            "/content/gdrive/MyDrive/ece5545/a2-Gibur13\n",
            "Already on 'main'\n",
            "Your branch is up to date with 'origin/main'.\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Counting objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 9 (delta 5), reused 5 (delta 5), pack-reused 4 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (9/9), 14.61 KiB | 20.00 KiB/s, done.\n",
            "From https://github.com/ML-HW-SYS/a2-Gibur13\n",
            "   baa4936..912b8e3  main       -> origin/main\n",
            "Updating baa4936..912b8e3\n",
            "Fast-forward\n",
            " 2_size_estimator_and_profiler.ipynb |  132 \u001b[32m++\u001b[m\u001b[31m--\u001b[m\n",
            " 3_training_and_analysis.ipynb       |   46 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " 5_quantization.ipynb                | 2285 \u001b[32m++++++++++++++++++++++++++++\u001b[m\u001b[31m---------------------------\u001b[m\n",
            " 3 files changed, 1270 insertions(+), 1193 deletions(-)\n",
            "/content/gdrive/MyDrive/ece5545\n"
          ]
        }
      ],
      "source": [
        "# Clone your github repo\n",
        "YOUR_TOKEN = token\n",
        "YOUR_HANDLE = handle\n",
        "BRANCH = \"main\"\n",
        "\n",
        "%mkdir /content/gdrive/MyDrive/ece5545\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "!git clone https://{YOUR_TOKEN}@github.com/ML-HW-SYS/a2-{YOUR_HANDLE}.git\n",
        "%cd /content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\n",
        "!git checkout {BRANCH}\n",
        "!git pull\n",
        "%cd /content/gdrive/MyDrive/ece5545\n",
        "\n",
        "PROJECT_ROOT = f\"/content/gdrive/MyDrive/ece5545/a2-{YOUR_HANDLE}\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "6eDWK1EVxVfN"
      },
      "outputs": [],
      "source": [
        "# This extension reloads all imports before running each cell\n",
        "%load_ext autoreload\n",
        "%autoreload 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4w6cL9l3xVfO"
      },
      "source": [
        "Please verify the cell below prints out the github repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "TQSeJ7fExVfP",
        "outputId": "10c3504e-0541-46a8-86e2-1fe981bf4c74",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_audio_preprocessing.ipynb\t     5_quantization.ipynb\t      README.md\n",
            "2_size_estimator_and_profiler.ipynb  6_pruning.ipynb\t\t      src\n",
            "3_training_and_analysis.ipynb\t     arduino_nano_33_ble_tutorial.md  tests\n",
            "4_model_conversion.ipynb\t     images\n"
          ]
        }
      ],
      "source": [
        "!ls '{PROJECT_ROOT}'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MM5gV8CNqd-s"
      },
      "source": [
        "### Install required packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "NLE4Xmakoq-4",
        "outputId": "44835657-c57d-4b47-f816-a1c843b1b909",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.5.1+cu124)\n",
            "Requirement already satisfied: torch==2.5.1 in /usr/local/lib/python3.11/dist-packages (from torchaudio) (2.5.1+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.1.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (2024.10.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.5.1->torchaudio)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.5.1->torchaudio) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.5.1->torchaudio) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.5.1->torchaudio) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m88.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m79.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ],
      "source": [
        "# Install libraries\n",
        "!pip install tqdm\n",
        "!pip install torchaudio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "55zgRQWVqu6n"
      },
      "source": [
        "### Import code dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "0pk0tnhRoq-5",
        "outputId": "dcbcc5ea-17ff-4d24-d1e7-da55c3de016f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model folders are created, \n",
            "PyTorch models will be saved in /content/gdrive/MyDrive/ece5545/models/torch_models, \n",
            "ONNX models will be saved in /content/gdrive/MyDrive/ece5545/models/onnx_models, \n",
            "TensorFlow Saved Models will be saved in /content/gdrive/MyDrive/ece5545/models/tf_models, \n",
            "TensorFlow Lite models will be saved in /content/gdrive/MyDrive/ece5545/models/tflite_models, \n",
            "TensorFlow Lite Micro models will be saved in /content/gdrive/MyDrive/ece5545/models/micro_models.\n"
          ]
        }
      ],
      "source": [
        "# Import libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import numpy as np\n",
        "import copy\n",
        "import os\n",
        "\n",
        "import sys\n",
        "\n",
        "# Adding assignment 2 to the system path\n",
        "# -- make sure this matches your git directory\n",
        "sys.path.insert(0, PROJECT_ROOT)\n",
        "\n",
        "# Import data_proc to use data processing functions\n",
        "import src.data_proc as data_proc\n",
        "\n",
        "# Import constants to use constants defined for training\n",
        "from src.constants import *\n",
        "\n",
        "# Set random seed\n",
        "# Make sure the shuffling and picking is deterministic\n",
        "# Note that different value of random_seed may change rate of variation in loss/accuracy during training\n",
        "# Using the same random seed value every time you rerun the notebook will\n",
        "# reproduce the training and testing results\n",
        "random_seed = RANDOM_SEED\n",
        "torch.manual_seed(random_seed)\n",
        "torch.cuda.manual_seed(random_seed)\n",
        "np.random.seed(random_seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPNyurTpoq_C"
      },
      "source": [
        "## 5.1 Define Quantization Functions\n",
        "\n",
        "There are some test cases in the `tests` folder to verify basic functionality of your implemented functions--these will be run automatically every time you check in your code. Additionally, we've left some simple tests in this notebook as well for you to try things out.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hTaCRrjeJgBR"
      },
      "source": [
        "#### TODO 0: Implement the backward pass of `ste_round` function in `src/quant.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VQy4ffsVPc8o"
      },
      "outputs": [],
      "source": [
        "# add a test if you like. There's already one under tests/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zOvGVC1CxVfR"
      },
      "source": [
        "\n",
        "#### TODO 1: Implement the `linear_quantize` function in `src/quant.py`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EXUIpaEMoq_C",
        "outputId": "8accde0c-5281-45be-bdaf-ace413c51c12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., -0., 0., 1.])\n"
          ]
        }
      ],
      "source": [
        "from src.quant import linear_quantize\n",
        "\n",
        "# Mini test case for linear_quantize\n",
        "with torch.no_grad():\n",
        "    x = torch.tensor([2, -0.5, 0., 1.])\n",
        "    scale = 1\n",
        "    zero = 0\n",
        "    y = linear_quantize(x, scale, zero)\n",
        "    print(y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvZ9zUycxVfR"
      },
      "source": [
        "#### TODO 2: Implement the `SymmetricQuantFunction` forward function in `src/quant.py`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RxiSaE_dxVfR",
        "outputId": "f4e70b95-2f2a-4cb0-92fb-a8c0e698f77b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([1., -0., 0., 1.], grad_fn=<SymmetricQuantFunctionBackward>)\n",
            "tensor([2., -0., 0., 2.])\n"
          ]
        }
      ],
      "source": [
        "from src.quant import SymmetricQuantFunction\n",
        "\n",
        "quant_f = SymmetricQuantFunction.apply\n",
        "\n",
        "x = torch.tensor([2, -0.5, 0., 1.])\n",
        "x.requires_grad = True\n",
        "bw = 2\n",
        "y = quant_f(x, bw, scale, zero)\n",
        "(y ** 2).sum().backward()\n",
        "\n",
        "print(y)\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7M_CmQ5-xVfS"
      },
      "source": [
        "#### TODO 3: Implement the `AsymmetricQuantFunction` forward function in `src/quant.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XQBP2WWtxVfS",
        "outputId": "4219d89b-0f0c-4be1-b887-a06a7aff70a5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([2., -0., 0., 1.], grad_fn=<AsymmetricQuantFunctionBackward>)\n",
            "tensor([4., -0., 0., 2.])\n"
          ]
        }
      ],
      "source": [
        "from src.quant import AsymmetricQuantFunction\n",
        "\n",
        "quant_f = AsymmetricQuantFunction.apply\n",
        "\n",
        "x = torch.tensor([2, -0.5, 0., 1.])\n",
        "x.requires_grad = True\n",
        "bw = 2\n",
        "y = quant_f(x, bw, scale, zero)\n",
        "(y ** 2).sum().backward()\n",
        "\n",
        "print(y)\n",
        "print(x.grad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EClG-iStxVfS"
      },
      "source": [
        "#### TODO 4: Finish the Implement of `get_quantization_params` function in `src/quant.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wtjXI5zjxVfS",
        "outputId": "bc516cc6-feec-427f-a88d-273d9da74805",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(tensor(0.2857), tensor(0))\n",
            "(tensor(0.1667), tensor(3, dtype=torch.int32))\n"
          ]
        }
      ],
      "source": [
        "from src.quant import QConfig\n",
        "\n",
        "qconfig = QConfig(quant_bits=4, is_symmetric=True)\n",
        "print(qconfig.get_quantization_params(x.min(), x.max()))\n",
        "\n",
        "qconfig = QConfig(quant_bits=4, is_symmetric=False)\n",
        "print(qconfig.get_quantization_params(x.min(), x.max()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLT8xONCoq_D"
      },
      "source": [
        "#### TODO 5: Implement the `quantize_weights_bias` function in `src/quant.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "1athpSf1oq_D",
        "outputId": "8bd7190e-3ea4-4a11-8aa8-9d546bd824a1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([ 7., -2.,  0.,  4.])\n",
            "tensor([ 7., -3.,  0.,  4.])\n"
          ]
        }
      ],
      "source": [
        "from src.quant import quantize_weights_bias, QConfig\n",
        "\n",
        "qconfig = QConfig(quant_bits=4, is_symmetric=True)\n",
        "\n",
        "w1 = nn.Parameter(torch.tensor([2, -0.5, 0., 1.]))\n",
        "qw1 = quantize_weights_bias(w1, qconfig)\n",
        "print(qw1.data)\n",
        "\n",
        "w2 = nn.Parameter(torch.tensor([2.5, -1, 0., 1.5]))\n",
        "qw2 = quantize_weights_bias(w2, qconfig)\n",
        "print(qw2.data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xnpBdnuBoq_E"
      },
      "source": [
        "## 5.2 Quantization Function for Linear and Convolution Layer\n",
        "\n",
        "#### TODO 6: Finish the implementation of `conv2d_linear_quantized` function in `src/quant.py`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XNN8WiYPoq_E",
        "outputId": "94e4f294-9a3f-4161-c1ff-32f7bd1af51b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.1000, 2.1000]], grad_fn=<AddmmBackward0>)\n",
            "tensor([[1.2429, 2.1000]], grad_fn=<AddmmBackward0>)\n"
          ]
        }
      ],
      "source": [
        "import torch.nn as nn\n",
        "from src.quant import QuantWrapper\n",
        "\n",
        "layer = nn.Linear(2, 2)\n",
        "layer.weight.data = torch.tensor([[0.1, 0.1], [-0.1, 0.1]]).view(2, 2).float()\n",
        "layer.bias.data = torch.tensor([1, 2]).view(*layer.bias.shape).float()\n",
        "x = torch.tensor([[0., 1]])\n",
        "print(layer(x))\n",
        "\n",
        "quant_layer = QuantWrapper(\n",
        "    layer,\n",
        "    QConfig(quant_bits=4, is_symmetric=True),\n",
        "    QConfig(quant_bits=4, is_symmetric=True),\n",
        "    QConfig(quant_bits=4, is_symmetric=True))\n",
        "print(quant_layer(x))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lax5KDyxoq_F"
      },
      "source": [
        "## 5.3 Prepare model for QAT (Quantization Aware Training)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yLFhKVStmTbG"
      },
      "source": [
        "### Get Audio Processor, Devices, Data Loader, and Model\n",
        "\n",
        "NOTE: This is identical to section 2.2 ."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "SBcLXHHim6ry",
        "outputId": "44b5101f-d09d-4f21-e008-4240c091fa25",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Audio_processor created\n",
            "Using cpu to run the training scrpit.\n",
            "Train size: 10556 Val size: 1333 Test size: 1368\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyConv(\n",
              "  (conv_reshape): Reshape(output_shape=(-1, 1, 49, 40))\n",
              "  (conv): Conv2d(1, 8, kernel_size=(10, 8), stride=(2, 2), padding=(5, 3))\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_reshape): Reshape(output_shape=(-1, 4000))\n",
              "  (fc): Linear(in_features=4000, out_features=4, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "# Create audio_processor\n",
        "# DATASET_DIR is defined in constants.py\n",
        "# HINT: In case loading data takes too long, move the dataset from gdrive to /content/ and change the path accordingly.\n",
        "audio_processor = data_proc.AudioProcessor(data_dir=DATASET_DIR)\n",
        "print(\"Audio_processor created\")\n",
        "\n",
        "# Define device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f'Using {device} to run the training scrpit.')\n",
        "\n",
        "# Define data loaders\n",
        "from src.loaders import make_data_loaders\n",
        "data_loaders = make_data_loaders(audio_processor, device)\n",
        "train_loader = data_loaders['training']\n",
        "test_loader = data_loaders['testing']\n",
        "valid_loader = data_loaders['validation']\n",
        "\n",
        "# Create a full precision (float32) TinyConv model\n",
        "from src.networks import TinyConv\n",
        "model_fp32 = TinyConv(model_settings=audio_processor.model_settings, \\\n",
        "    n_input=1, n_output=audio_processor.num_labels)\n",
        "torch.save(model_fp32.state_dict(), os.path.join(TORCH_DIR, \"tinyconv_float32_init_seed0_90.28%_0.pt\"))\n",
        "model_fp32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7wcn11MxVfT"
      },
      "source": [
        "### Load Pretrained Model for Quantization Aware Finetuning\n",
        "\n",
        "In this notebook, we will load the previously trained 32-bits float model to finetune it in a quantizaiton-aware way.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TORCH_DIR = PROJECT_ROOT"
      ],
      "metadata": {
        "id": "V7Z50DiT6zlq"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "e_QYYl4oxVfT",
        "outputId": "3b6e8ec1-9793-42cb-f653-37f16cad838b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1_audio_preprocessing.ipynb\t     arduino_nano_33_ble_tutorial.md\n",
            "2_size_estimator_and_profiler.ipynb  images\n",
            "3_training_and_analysis.ipynb\t     README.md\n",
            "4_model_conversion.ipynb\t     src\n",
            "5_quantization.ipynb\t\t     tests\n",
            "6_pruning.ipynb\t\t\t     tinyconv_float32_init_seed0_90.28%_0.pt\n"
          ]
        }
      ],
      "source": [
        "!ls {TORCH_DIR}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys6gVPF6xVfT"
      },
      "source": [
        "### **TODO: Replace the torch_path model with the model you created in the last section.**\n",
        "\n",
        "You can find the name of your file in `TORCH_DIR` under the folder icon to the left. (Or from running the tab above)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "98TcWxPWxVfU",
        "outputId": "22809de2-1cfe-42ba-f273-c38848d1d0d7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-24-40e93926a152>:5: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model_fp32.load_state_dict(torch.load(torch_path))\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TinyConv(\n",
              "  (conv_reshape): Reshape(output_shape=(-1, 1, 49, 40))\n",
              "  (conv): Conv2d(1, 8, kernel_size=(10, 8), stride=(2, 2), padding=(5, 3))\n",
              "  (relu): ReLU()\n",
              "  (dropout): Dropout(p=0.5, inplace=False)\n",
              "  (fc_reshape): Reshape(output_shape=(-1, 4000))\n",
              "  (fc): Linear(in_features=4000, out_features=4, bias=True)\n",
              "  (softmax): Softmax(dim=1)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ],
      "source": [
        "# TODO: Replace me!\n",
        "torch_path = os.path.join(TORCH_DIR, \"tinyconv_float32_init_seed0_90.28%_0.pt\")\n",
        "\n",
        "# Load model\n",
        "model_fp32.load_state_dict(torch.load(torch_path))\n",
        "model_fp32"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s6RTZp_yoq_F"
      },
      "source": [
        "### Define settings for weight and activation quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "KrTGvZMtoq_F"
      },
      "outputs": [],
      "source": [
        "# We choose 4 bit quantization as an example because accuracy improvements will\n",
        "# be more obvious with 4-bit or lower bit quantization\n",
        "QUANT_BITS = 4\n",
        "# Settings for activations quantization: n-bit asymmetric quantization\n",
        "a_qconfig = QConfig(quant_bits=QUANT_BITS, is_symmetric=False)\n",
        "# Settings for weights quantization: n-bit symmetric quantization\n",
        "w_qconfig = QConfig(quant_bits=QUANT_BITS, is_symmetric=True)\n",
        "# Settings for bias quantization: n-bit symmetric quantization\n",
        "b_qconfig = QConfig(quant_bits=QUANT_BITS, is_symmetric=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5FD7l9cNoq_F"
      },
      "source": [
        "### Prepare quantization aware training model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "capV1g7loq_F",
        "outputId": "28472463-56e9-478e-c065-f4e18f753a11",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TinyConv(\n",
            "  (conv_reshape): Reshape(output_shape=(-1, 1, 49, 40))\n",
            "  (conv): QuantWrapper(\n",
            "    (module): Conv2d(1, 8, kernel_size=(10, 8), stride=(2, 2), padding=(5, 3))\n",
            "  \t(activation): quant_bits=4, quant_mode=asymmetric, prev_scale=None, prev_zeropoint=None, prev_min=None, prev_max=None  \n",
            "  \t(weight): quant_bits=4, quant_mode=symmetric, prev_scale=None, prev_zeropoint=None, prev_min=None, prev_max=None  \n",
            "  \t(bias): quant_bits=4, quant_mode=symmetric, prev_scale=None, prev_zeropoint=None, prev_min=None, prev_max=None  \n",
            "  )\n",
            "  (relu): ReLU()\n",
            "  (dropout): Dropout(p=0.5, inplace=False)\n",
            "  (fc_reshape): Reshape(output_shape=(-1, 4000))\n",
            "  (fc): QuantWrapper(\n",
            "    (module): Linear(in_features=4000, out_features=4, bias=True)\n",
            "  \t(activation): quant_bits=4, quant_mode=asymmetric, prev_scale=None, prev_zeropoint=None, prev_min=None, prev_max=None  \n",
            "  \t(weight): quant_bits=4, quant_mode=symmetric, prev_scale=None, prev_zeropoint=None, prev_min=None, prev_max=None  \n",
            "  \t(bias): quant_bits=4, quant_mode=symmetric, prev_scale=None, prev_zeropoint=None, prev_min=None, prev_max=None  \n",
            "  )\n",
            "  (softmax): Softmax(dim=1)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from src.quant import quantize_model\n",
        "qat_model_nbit = quantize_model(\n",
        "    model_fp32, a_qconfig=a_qconfig, w_qconfig=w_qconfig, b_qconfig=b_qconfig)\n",
        "\n",
        "# Print to see the model prepared for QAT\n",
        "print(qat_model_nbit)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkHtQEQXoq_G"
      },
      "source": [
        "##  5.4 Finetuning\n",
        "\n",
        "In this training, we will finetune the 32-bits float pretrained model. The goal is to finetune the weights of the 32-bits float model such that the resulted model will have better accuracy after quantization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvZ9g2QlxVfU"
      },
      "source": [
        "### Quantization Aware Finetuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "fCC9s6dUoq_H",
        "scrolled": false,
        "outputId": "a8c3466d-6a7e-40f7-df02-bb803f00a05a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 487,
          "referenced_widgets": [
            "2783c6b25c5747ae90c2037783665101",
            "55844d6cc9c748969944167cc0d8743d",
            "0407bf49d2344305b508b5136dac20c5",
            "4d3117785bd44e3c948d9a3e3ca0f7a4",
            "0ee50cd2be7f41a2a4879bed74d32367",
            "5098ccc4b2134dfe9b4388176e4e3642",
            "eabb5308bdf9477db0e6a27dadbdef22",
            "00af52a24d7745f0b771bb751541f9a8",
            "d6dfcf5f957249929bdc216c492889f5",
            "7360f47f446c47c7b51c0b77684052a6",
            "60b012117e524fcfa44a0229e9043ca7"
          ]
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "#batches: 106 \n",
            "#epochs: 30 \n",
            "#total training steps: 3180\n",
            "{'state': {}, 'param_groups': [{'lr': 0.001, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0.0001, 'amsgrad': False, 'maximize': False, 'foreach': None, 'capturable': False, 'differentiable': False, 'fused': None, 'params': [0, 1, 2, 3]}]}\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/30 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2783c6b25c5747ae90c2037783665101"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val Acc Epoch 1 = 35.93%, Train loss = 20.302\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-89d4c9449bc8>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTORCH_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"quant_checkpoint.pt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0mqat_model_nbit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m run_training(\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mqat_model_nbit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loaders\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mn_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_interval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_interval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-89d4c9449bc8>\u001b[0m in \u001b[0;36mrun_training\u001b[0;34m(model, data_loaders, n_epoch, log_interval, optimizer, scheduler, save_interval, resume, checkpoint_path, verbose)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompleted_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epoch\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m             \u001b[0mtrain_iters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_loaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'training'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_loaders\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m             test(test_loader, model, device,\n\u001b[1;32m     31\u001b[0m                  epoch=None, loader_type='Test')\n",
            "\u001b[0;32m/content/gdrive/MyDrive/ece5545/a2-Gibur13/src/train_val_test_utils.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, loaders, optimizer, epoch, device, verbose)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0mnum_batches\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m         \u001b[0;31m# Calculate iteration_index\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0miteration_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_batches\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    699\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    700\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 701\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    702\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    703\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/content/gdrive/MyDrive/ece5545/a2-Gibur13/src/loaders.py\u001b[0m in \u001b[0;36mcollate_fn_\u001b[0;34m(batch)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# added to the audio data except for silence.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mdict\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             data, label = audio_processor.get_data_from_file(\n\u001b[0m\u001b[1;32m     27\u001b[0m                 \u001b[0mdict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBACKGROUND_FREQUENCY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBACKGROUND_VOLUME_RANGE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m                 TIME_SHIFT_SAMPLE, mode)\n",
            "\u001b[0;32m/content/gdrive/MyDrive/ece5545/a2-Gibur13/src/data_proc.py\u001b[0m in \u001b[0;36mget_data_from_file\u001b[0;34m(self, sample, background_frequency, background_volume_range, time_shift, mode)\u001b[0m\n\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m     \u001b[0;31m# run the graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 331\u001b[0;31m     self.prepare_processing_graph(wav_filename, foreground_volume, time_shift_offset, \\\n\u001b[0m\u001b[1;32m    332\u001b[0m         time_shift_padding, background_data, background_volume)\n\u001b[1;32m    333\u001b[0m     \u001b[0mdata_tensor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/gdrive/MyDrive/ece5545/a2-Gibur13/src/data_proc.py\u001b[0m in \u001b[0;36mprepare_processing_graph\u001b[0;34m(self, wav_filename, foreground_volume, time_shift_offset, time_shift_padding, background_data, background_volume)\u001b[0m\n\u001b[1;32m    535\u001b[0m     \u001b[0;31m# loads and decode a WAVE file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0mdesired_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_settings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'desired_samples'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 537\u001b[0;31m     \u001b[0mwaveform\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwav_filename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdesired_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    538\u001b[0m     \u001b[0mtotal_num_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwaveform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    539\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtotal_num_samples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mdesired_samples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/_backend/utils.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[0m\n\u001b[1;32m    203\u001b[0m         \"\"\"\n\u001b[1;32m    204\u001b[0m         \u001b[0mbackend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdispatcher\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 205\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m    295\u001b[0m         \u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m4096\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m     ) -> Tuple[torch.Tensor, int]:\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mload_audio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchannels_first\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mstaticmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torchaudio/_backend/ffmpeg.py\u001b[0m in \u001b[0;36mload_audio\u001b[0;34m(src, frame_offset, num_frames, convert, channels_first, format, buffer_size)\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"read\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mformat\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"vorbis\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mformat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"ogg\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m     \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorchaudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m     \u001b[0msample_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_src_stream_info\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdefault_audio_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0mfilter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_load_filter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_frames\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torio/io/_streaming_media_decoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, format, option, buffer_size)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoderFileObj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 526\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mffmpeg_ext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mStreamingMediaDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormpath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mformat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moption\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    527\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0mi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_be\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_best_audio_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import time\n",
        "from src.train_val_test_utils import train, test\n",
        "from src.train_val_test_utils import create_optimizer\n",
        "\n",
        "\n",
        "def run_training(model, data_loaders, n_epoch, log_interval, optimizer, scheduler=None,\n",
        "                 save_interval=1, resume=True, checkpoint_path=None, verbose=False):\n",
        "    test_loader = data_loaders['testing']\n",
        "    with tqdm(total=n_epoch) as pbar:\n",
        "        completed_epoch = 1\n",
        "        if resume:\n",
        "            try:\n",
        "                #continue training with previous model if one exists\n",
        "                if checkpoint_path is None:\n",
        "                    raise ValueError\n",
        "                checkpoint = torch.load(checkpoint_path)\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                if scheduler is not None:\n",
        "                    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "                completed_epoch = checkpoint[\"epoch\"] + 1\n",
        "                model.eval()\n",
        "                pbar.update(completed_epoch)\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "        for epoch in range(completed_epoch, n_epoch + 1):\n",
        "            train_iters = len(data_loaders['training'])\n",
        "            train(model, data_loaders, optimizer, epoch, device, verbose)\n",
        "            test(test_loader, model, device,\n",
        "                 epoch=None, loader_type='Test')\n",
        "\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "            #checkpoint the model every run\n",
        "            if epoch % save_interval == 0 and checkpoint_path is not None:\n",
        "                torch.save({\n",
        "                    'epoch': epoch,\n",
        "                    'model_state_dict': model.state_dict(),\n",
        "                    'optimizer_state_dict': optimizer.state_dict(),\n",
        "                    'scheduler_state_dict': scheduler.state_dict() if scheduler is not None else None\n",
        "                }, checkpoint_path)\n",
        "\n",
        "            # Update epoch pbar\n",
        "            pbar.update(1)\n",
        "\n",
        "\n",
        "verbose = False\n",
        "log_interval = 100\n",
        "num_batches = len(train_loader)\n",
        "n_epoch = 30\n",
        "print(f'#batches: {num_batches} \\n#epochs: {n_epoch} \\n#total training steps: {num_batches * n_epoch}')\n",
        "\n",
        "# Create optimizer\n",
        "optimizer_quant = create_optimizer(model=qat_model_nbit, learning_rate=0.001)\n",
        "print(optimizer_quant.state_dict())\n",
        "\n",
        "checkpoint_path = os.path.join(TORCH_DIR, \"quant_checkpoint.pt\")\n",
        "qat_model_nbit.to(device)\n",
        "run_training(\n",
        "    model=qat_model_nbit, data_loaders=data_loaders,\n",
        "    n_epoch=n_epoch, log_interval=log_interval,\n",
        "    optimizer=optimizer_quant, scheduler=None,\n",
        "    resume=False,\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    verbose=verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rj8xFMjqxVfV"
      },
      "source": [
        "### Finetune the Float Model\n",
        "\n",
        "For fair comparison, we conduct the same funetuning for the float model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hx9VthdKxVfV",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Create optimizer\n",
        "optimizer_fp32 = create_optimizer(model=model_fp32, learning_rate=0.0001)\n",
        "\n",
        "checkpoint_path = os.path.join(TORCH_DIR, \"fp32_finetune_checkpoint.pt\")\n",
        "model_fp32.to(device)\n",
        "run_training(\n",
        "    model=model_fp32, data_loaders=data_loaders,\n",
        "    n_epoch=n_epoch, log_interval=log_interval,\n",
        "    optimizer=optimizer_fp32, scheduler=None,\n",
        "    resume=False,\n",
        "    checkpoint_path=checkpoint_path,\n",
        "    verbose=verbose\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOIj0QUOoq_H"
      },
      "source": [
        "## 5.5 Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "95C3tSGNoq_H"
      },
      "source": [
        "We will compute the accuracy of the finetuned model in train/val/test set in this section.\n",
        "Note that this is not the final accuracy we want the model to perform well on.\n",
        "We would like our quantized-aware-finetuned model to perform well when quantized into integer.\n",
        "But the training/validation/testing accuracy of these model in quantization simulation model is still worth looking at for sanity checking purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0eFxrBqCoq_H",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from src.train_val_test_utils import plot_acc\n",
        "\n",
        "test_time_data_loaders = make_data_loaders(\n",
        "    audio_processor, device,\n",
        "    test_batch_size=1, valid_batch_size=1,\n",
        "    num_workers=0\n",
        ")\n",
        "\n",
        "plot_acc(\n",
        "    test_time_data_loaders['training'], qat_model_nbit, audio_processor, device,\n",
        "    \"Training\", 'n-bit Quantized TinyConv', \"float\")\n",
        "plot_acc(\n",
        "    test_time_data_loaders['validation'], qat_model_nbit, audio_processor, device,\n",
        "    \"Validation\", 'n-bit Quantized TinyConv', \"float\")\n",
        "plot_acc(\n",
        "    test_time_data_loaders['testing'], qat_model_nbit, audio_processor, device,\n",
        "    'Testing', 'n-bit Quantized TinyConv', \"float\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnJHV-H_xVfV"
      },
      "outputs": [],
      "source": [
        "plot_acc(\n",
        "    test_time_data_loaders['training'], model_fp32, audio_processor, device,\n",
        "    \"Training\", 'FP32 FT TinyConv', \"float\")\n",
        "plot_acc(\n",
        "    test_time_data_loaders['validation'], model_fp32, audio_processor, device,\n",
        "    \"Validation\", 'FP32 FT TinyConv', \"float\")\n",
        "acc = plot_acc(\n",
        "    test_time_data_loaders['testing'], model_fp32, audio_processor, device,\n",
        "    'Testing', 'FP32 FT TinyConv', \"float\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AehU-BsJgBV"
      },
      "source": [
        "## 5.6 Saving the Trained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "agtFnTuTJgBV"
      },
      "outputs": [],
      "source": [
        "from src.train_val_test_utils import choose_name\n",
        "from src.quant import dequantize_model\n",
        "\n",
        "# Save the qat model\n",
        "qat_model_nbit_float = dequantize_model(qat_model_nbit)\n",
        "file_name = choose_name(\"quant\")\n",
        "# You can also define your own path\n",
        "qat_torch_path = os.path.join(TORCH_DIR, f'(QAT{QUANT_BITS}bit){file_name}.pt')\n",
        "# Save the trained n-bit qat pytorch model to PATH\n",
        "torch.save(qat_model_nbit.state_dict(), qat_torch_path)\n",
        "qat_torch_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mij775vWoq_I"
      },
      "source": [
        "## 5.7 Understanding and Evaluate the Effectiveness of Quantization-Aware Training (QAT)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUCINlWNtQB-"
      },
      "source": [
        "### Model conversion: Quantized/Float Fine-tuning Model Converted to Integer Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odKeXjOSoq_I"
      },
      "outputs": [],
      "source": [
        "from src.quant import dequantize_model\n",
        "from src.quant_conversion import convert_to_int"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ssFRMhToq_I"
      },
      "outputs": [],
      "source": [
        "# Convert to quantized model\n",
        "\n",
        "# Quantized integer model of qat_model_nbit (quantized aware finetuning model)\n",
        "int_model_nbit = convert_to_int(\n",
        "    qat_model_nbit, QUANT_BITS, dtype=torch.int32)\n",
        "\n",
        "# Post quantized model of model_fp32 (full-precision finetuned model)\n",
        "post_quant_model = convert_to_int(\n",
        "    model_fp32, QUANT_BITS, dtype=torch.int32)\n",
        "\n",
        "# Floating point models of the qat_model_nbit, without QuantWrappers\n",
        "float_model_nbit = dequantize_model(qat_model_nbit)\n",
        "\n",
        "print(int_model_nbit)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "post_quantized_model_float = quantize_model(\n",
        "    model_fp32, a_qconfig=a_qconfig, w_qconfig=w_qconfig, b_qconfig=b_qconfig)\n",
        "plot_acc(\n",
        "    test_time_data_loaders['training'], post_quantized_model_float, audio_processor, device,\n",
        "    \"Training\", 'Post Quantized 8-bit TinyConv', \"float\")\n",
        "plot_acc(\n",
        "    test_time_data_loaders['validation'], post_quantized_model_float, audio_processor, device,\n",
        "    \"Validation\", 'Post Quantized 8-bit TinyConv', \"float\")\n",
        "plot_acc(\n",
        "    test_time_data_loaders['testing'], post_quantized_model_float, audio_processor, device,\n",
        "    'Testing', 'Post Quantized 8-bit TinyConv', \"float\")"
      ],
      "metadata": {
        "id": "z3GIyFRc7RMp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jh2NGspeoq_I"
      },
      "outputs": [],
      "source": [
        "from src.quant_conversion import print_features\n",
        "\n",
        "# Select a sample data to see the features of it\n",
        "sample_data, _ = audio_processor.get_data_from_file(\n",
        "    audio_processor.data_index['testing'][0], BACKGROUND_FREQUENCY,\n",
        "    BACKGROUND_VOLUME_RANGE, TIME_SHIFT_SAMPLE, 'testing')\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"Features from Quantized QAT Model\")\n",
        "print(\"-\" * 80)\n",
        "print_features(sample_data, int_model_nbit, 'Quantized QAT Model')\n",
        "print()\n",
        "print(\"=\" * 80)\n",
        "print(\"Features from Model fp32\")\n",
        "print(\"-\" * 80)\n",
        "print_features(sample_data, model_fp32, \"Model fp32\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMm7grfnoq_I"
      },
      "source": [
        "### Compare the Performance Between Integer Models from Float/Quantized-Aware Finetuning"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**4 bit quantization**"
      ],
      "metadata": {
        "id": "uikaJGlNnlhR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kHJaISbYoq_J"
      },
      "outputs": [],
      "source": [
        "from src.quant_conversion import compare_model, compare_model_mse\n",
        "\n",
        "# Compare differences in predictions\n",
        "# 4 bit quantization\n",
        "# QAT trained floating point model vs. integer model converted from the QAT model\n",
        "# Percentage of same predictions shows how \"quantization aware\" the float point model is\n",
        "_ = compare_model(test_loader, float_model_nbit, int_model_nbit)\n",
        "_ = compare_model_mse(test_loader, float_model_nbit, int_model_nbit)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNikQddjoq_J"
      },
      "outputs": [],
      "source": [
        "from src.quant_conversion import compare_model, compare_model_mse\n",
        "# 4 bit quantization\n",
        "# Float32 model vs. integer model converted from the float32 model using post training quantization\n",
        "_ = compare_model(test_loader, model_fp32, post_quant_model)\n",
        "_ = compare_model_mse(test_loader, model_fp32, post_quant_model)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**6 bit quantization**"
      ],
      "metadata": {
        "id": "z4AVcDrrng-W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.quant_conversion import compare_model, compare_model_mse\n",
        "\n",
        "# Compare differences in predictions\n",
        "# 6 bit quantization\n",
        "# QAT trained floating point model vs. integer model converted from the QAT model\n",
        "# Percentage of same predictions shows how \"quantization aware\" the float point model is\n",
        "_ = compare_model(test_loader, float_model_nbit, int_model_nbit)\n",
        "_ = compare_model_mse(test_loader, float_model_nbit, int_model_nbit)"
      ],
      "metadata": {
        "id": "QdpgKrDDnTcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.quant_conversion import compare_model, compare_model_mse\n",
        "# 6 bit quantization\n",
        "# Float32 model vs. integer model converted from the float32 model using post training quantization\n",
        "_ = compare_model(test_loader, model_fp32, post_quant_model)\n",
        "_ = compare_model_mse(test_loader, model_fp32, post_quant_model)"
      ],
      "metadata": {
        "id": "5PPiznNAnU2t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2 bit quantization**"
      ],
      "metadata": {
        "id": "h76Wz6MUnaq9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.quant_conversion import compare_model, compare_model_mse\n",
        "\n",
        "# Compare differences in predictions\n",
        "# 2 bit quantization\n",
        "# QAT trained floating point model vs. integer model converted from the QAT model\n",
        "# Percentage of same predictions shows how \"quantization aware\" the float point model is\n",
        "_ = compare_model(test_loader, float_model_nbit, int_model_nbit)\n",
        "_ = compare_model_mse(test_loader, float_model_nbit, int_model_nbit)"
      ],
      "metadata": {
        "id": "_Wy-_ZvnnWYz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.quant_conversion import compare_model, compare_model_mse\n",
        "# 2 bit quantization\n",
        "# Float32 model vs. integer model converted from the float32 model using post training quantization\n",
        "_ = compare_model(test_loader, model_fp32, post_quant_model)\n",
        "_ = compare_model_mse(test_loader, model_fp32, post_quant_model)"
      ],
      "metadata": {
        "id": "iT0oxY-FnYwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**8 bit quantization**"
      ],
      "metadata": {
        "id": "SsVo2RetnsJM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from src.quant_conversion import compare_model, compare_model_mse\n",
        "\n",
        "# Compare differences in predictions\n",
        "# 2 bit quantization\n",
        "# QAT trained floating point model vs. integer model converted from the QAT model\n",
        "# Percentage of same predictions shows how \"quantization aware\" the float point model is\n",
        "_ = compare_model(test_loader, float_model_nbit, int_model_nbit)\n",
        "_ = compare_model_mse(test_loader, float_model_nbit, int_model_nbit)"
      ],
      "metadata": {
        "id": "mBZcVzobnuj0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from src.quant_conversion import compare_model, compare_model_mse\n",
        "# 2 bit quantization\n",
        "# Float32 model vs. integer model converted from the float32 model using post training quantization\n",
        "_ = compare_model(test_loader, model_fp32, post_quant_model)\n",
        "_ = compare_model_mse(test_loader, model_fp32, post_quant_model)"
      ],
      "metadata": {
        "id": "wqYSeCJZnwRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ab1e0395"
      },
      "source": [
        "## Extra Credit: Minifloat Quantization & QAT vs PTQ Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "baf0fca5"
      },
      "source": [
        "### **Implementing Minifloat Quantization**\n",
        "Minifloat quantization provides a flexible tradeoff between range and precision. We implement quantization with `4` exponent bits and `3` mantissa bits, which balances accuracy and efficiency for embedded ML."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7c0ba034"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class MinifloatQuantFunction(torch.autograd.Function):\n",
        "    \"\"\"Custom autograd function for minifloat quantization.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, exponent_bits=4, mantissa_bits=3):\n",
        "        scale = 2 ** mantissa_bits  # Define scale based on mantissa bits\n",
        "        quantized = torch.round(x * scale) / scale  # Apply quantization\n",
        "        return quantized\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return grad_output.clone(), None, None\n",
        "\n",
        "def minifloat_quantize(x, exponent_bits=4, mantissa_bits=3):\n",
        "    return MinifloatQuantFunction.apply(x, exponent_bits, mantissa_bits)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b2ff40b2"
      },
      "source": [
        "### **Training with Minifloat QAT**\n",
        "We apply minifloat quantization during training to preserve accuracy in low-bit quantization scenarios."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fcef4783"
      },
      "outputs": [],
      "source": [
        "# Define Minifloat QAT Configuration\n",
        "class MinifloatQConfig(QConfig):\n",
        "    def quantize_with_params(self, x, scale, zero_point, fake_quantize=False):\n",
        "        x_q = minifloat_quantize(x, exponent_bits=4, mantissa_bits=3)\n",
        "        if fake_quantize:\n",
        "            x_q = (x_q - zero_point) * scale\n",
        "        return x_q\n",
        "\n",
        "# Apply Minifloat QAT to the model\n",
        "minifloat_qconfig = MinifloatQConfig()\n",
        "qat_model_minifloat = quantize_model(model_fp32, minifloat_qconfig, minifloat_qconfig)\n",
        "\n",
        "# Train Minifloat QAT model\n",
        "import torch.optim as optim\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(qat_model_minifloat.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(10):  # Train for 10 epochs\n",
        "    qat_model_minifloat.train()\n",
        "    running_loss = 0.0\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = qat_model_minifloat(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch [{epoch+1}/10], Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Save trained model\n",
        "torch.save(qat_model_minifloat.state_dict(), \"qat_model_minifloat.pth\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81eff226"
      },
      "source": [
        "### **Evaluating Minifloat QAT Model**\n",
        "We now evaluate how minifloat quantization affects accuracy compared to PTQ and standard QAT."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d00b1f72"
      },
      "outputs": [],
      "source": [
        "# Evaluate Minifloat QAT model\n",
        "minifloat_acc = evaluate_model(qat_model_minifloat, test_loader)\n",
        "print(f\"Minifloat QAT Accuracy (e=4, m=3): {minifloat_acc:.2f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "219e5397"
      },
      "source": [
        "### **Comparing PTQ, QAT, and Minifloat**\n",
        "We compare the accuracy of **Post-Training Quantization (PTQ), Quantization-Aware Training (QAT), and Minifloat QAT** by plotting their accuracy across different bit-widths."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63831941"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Collect accuracy data\n",
        "quant_types = [\"PTQ\", \"QAT\", \"Minifloat\"]\n",
        "accuracies = [accuracy_ptq[-1], accuracy_qat[-1], minifloat_acc]\n",
        "\n",
        "# Plot accuracy comparison\n",
        "plt.figure(figsize=(6, 4))\n",
        "plt.bar(quant_types, accuracies, color=['blue', 'green', 'red'])\n",
        "plt.ylabel(\"Accuracy (%)\")\n",
        "plt.title(\"Accuracy Comparison: PTQ vs. QAT vs. Minifloat\")\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a1a0cecf"
      },
      "source": [
        "### **Analysis of Minifloat Quantization**\n",
        "- **Why 4 exponent bits and 3 mantissa bits?**\n",
        "  - 4 exponent bits provide a reasonable numerical range for inference.\n",
        "  - 3 mantissa bits ensure sufficient precision without excessive memory usage.\n",
        "  - This balance ensures accurate inference on low-power microcontrollers.\n",
        "\n",
        "- **Comparison of Results**:\n",
        "  - **PTQ performs well at high bit-widths but loses accuracy at 2-bit and 4-bit quantization.**\n",
        "  - **QAT maintains higher accuracy for lower bit-widths.**\n",
        "  - **Minifloat QAT provides an additional tradeoff: better accuracy than PTQ, but slightly lower than integer QAT in some cases.**\n",
        "\n",
        "- **Final Takeaway**:\n",
        "  - **If integer quantization is too restrictive, minifloat offers a flexible alternative for embedded ML applications.**\n",
        "  - It works particularly well in environments where **floating-point operations are partially supported.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIty7jgJ6iBJ"
      },
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "5_quantization.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "interpreter": {
      "hash": "eed6bdaff5cc743acd241b8f3fe4c5dc3266475018a66ac615ca19ad2f28387d"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "2783c6b25c5747ae90c2037783665101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_55844d6cc9c748969944167cc0d8743d",
              "IPY_MODEL_0407bf49d2344305b508b5136dac20c5",
              "IPY_MODEL_4d3117785bd44e3c948d9a3e3ca0f7a4"
            ],
            "layout": "IPY_MODEL_0ee50cd2be7f41a2a4879bed74d32367"
          }
        },
        "55844d6cc9c748969944167cc0d8743d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5098ccc4b2134dfe9b4388176e4e3642",
            "placeholder": "​",
            "style": "IPY_MODEL_eabb5308bdf9477db0e6a27dadbdef22",
            "value": "  3%"
          }
        },
        "0407bf49d2344305b508b5136dac20c5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_00af52a24d7745f0b771bb751541f9a8",
            "max": 30,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d6dfcf5f957249929bdc216c492889f5",
            "value": 1
          }
        },
        "4d3117785bd44e3c948d9a3e3ca0f7a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7360f47f446c47c7b51c0b77684052a6",
            "placeholder": "​",
            "style": "IPY_MODEL_60b012117e524fcfa44a0229e9043ca7",
            "value": " 1/30 [01:46&lt;41:24, 85.69s/it]"
          }
        },
        "0ee50cd2be7f41a2a4879bed74d32367": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5098ccc4b2134dfe9b4388176e4e3642": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eabb5308bdf9477db0e6a27dadbdef22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00af52a24d7745f0b771bb751541f9a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d6dfcf5f957249929bdc216c492889f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7360f47f446c47c7b51c0b77684052a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "60b012117e524fcfa44a0229e9043ca7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}